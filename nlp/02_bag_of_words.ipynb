{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow https://www.freecodecamp.org/news/an-introduction-to-bag-of-words-and-how-to-code-it-in-python-for-nlp-282e87a9da04/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re \n",
    "import nltk\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('gs://cytora-user-wout/News_Category_Dataset_v2.json', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Tokenize a sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_extraction(sentence):\n",
    "    ignore = list(stopwords_set)    \n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split()    \n",
    "    cleaned_text = [w.lower() for w in words if w.lower() not in ignore]    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark Cuban Slams U.S. Plan To Give Chinese Billionaire Monopoly On Cancer Drug\n",
      "['mark', 'cuban', 'slams', 'u', 'plan', 'give', 'chinese', 'billionaire', 'monopoly', 'cancer', 'drug']\n",
      "\n",
      "Donald Trump Sells Christmas Tree Ornament, Gets People Out Of The Holiday Spirit\n",
      "['donald', 'trump', 'sells', 'christmas', 'tree', 'ornament', 'gets', 'people', 'holiday', 'spirit']\n",
      "\n",
      "Donald Trump Can Expect A Lot Of American Taxpayers Outside The White House April 15\n",
      "['donald', 'trump', 'expect', 'lot', 'american', 'taxpayers', 'outside', 'white', 'house', 'april', '15']\n",
      "\n",
      "Get Lucky Now: 4 Simple Ways to Jump Start Your Successes in Life\n",
      "['get', 'lucky', '4', 'simple', 'ways', 'jump', 'start', 'successes', 'life']\n",
      "\n",
      "'The Birth Of A Nation' Is The Toast Of Sundance\n",
      "['birth', 'nation', 'toast', 'sundance']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sen in df['headline'].sample(5):\n",
    "    print(sen)\n",
    "    print(word_extraction(sen))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Words like `isn't` and `U.S.` aren't extracted properly, maybe in case of `isn't` it doesn't matter as it will just get mapped to `isn`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Apply tokenization to all sentences\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):    \n",
    "    words = []    \n",
    "    for sentence in sentences:        \n",
    "        w = word_extraction(sentence)        \n",
    "        words.extend(w)            \n",
    "        words = sorted(list(set(words)))    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113846    Skateboarders Turn Damaged Street Into Ramp Af...\n",
       "37952     Everything You Need To Know Before Sunday's Go...\n",
       "Name: headline, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['california',\n",
       " 'damaged',\n",
       " 'earthquake',\n",
       " 'everything',\n",
       " 'globes',\n",
       " 'golden',\n",
       " 'including',\n",
       " 'know',\n",
       " 'need',\n",
       " 'ramp',\n",
       " 'skateboarders',\n",
       " 'street',\n",
       " 'sunday',\n",
       " 'turn',\n",
       " 'win']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample2 = df['headline'].sample(2)\n",
    "display(sample2)\n",
    "tokenize(sample2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build vocabulary and generate vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bow(allsentences):        \n",
    "    vocab = tokenize(allsentences)\n",
    "    vector_space = pd.DataFrame()\n",
    "    print(f'Done with tokenising vocabular of length: {len(vocab)}')\n",
    "\n",
    "    for sentence in allsentences:\n",
    "        words = word_extraction(sentence)\n",
    "        bag_vector = np.zeros(len(vocab))\n",
    "        for w in words:\n",
    "            for i, word in enumerate(vocab):\n",
    "                if word == w:\n",
    "                    bag_vector[i] += 1\n",
    "            vector_space[sentence] = np.array(bag_vector)\n",
    "#     vector_space = \n",
    "    return vocab, vector_space.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** why is the world `the` still in there? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sample(10000) # REMOVE THIS STEP MemoryError when ran on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with tokenising vocabular of length: 14679\n",
      "4.0 minutes to run\n"
     ]
    }
   ],
   "source": [
    "now = pd.datetime.now() \n",
    "\n",
    "vocab, vector_space = generate_bow(df['headline'])\n",
    "\n",
    "diff = pd.datetime.now() - now\n",
    "print(f'{diff.seconds / 60:.1f} minutes to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vector_space.columns = [str(x) for x in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_space.to_csv('data/02_bag_of_words.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** fix this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_space[\n",
    "#     (vector_space['04'] == 1) |\n",
    "#     (vector_space['000'] == 1) |\n",
    "#     (vector_space['1'] == 1) |\n",
    "#     (vector_space['102'] == 1) \n",
    "# ].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_space = vector_space.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** maybe remove vectors that represent < 6 words, this will be random anyway "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['category'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK IF STILL NAMED INDEX\n",
    "\n",
    "Otherwise change name in following cell \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_space.rename(columns={'level_0': 'head_line'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'headline': 'head_line'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 6)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9973, 14680)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is `category` a already in the bag of words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = vector_space.merge(df[['category', 'head_line']], suffixes=('', '_target'), how='inner', on='head_line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('data/02_bag_of_words.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
